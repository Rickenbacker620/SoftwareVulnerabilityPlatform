from calendar import c
from itertools import zip_longest
from pprint import pprint
import re
from typing import Optional
from numpy import sort
from pydantic import BaseModel
import httpx
import pydantic
from pymongo import MongoClient
from pymongo.database import Database
from bs4 import BeautifulSoup
import csv
import json
import spacy

from pyparsing import restOfLine


def get_database() -> Database:
    CONNECTION_STRING = "mongodb://localhost"
    client = MongoClient(CONNECTION_STRING)
    return client["svp"]


db = get_database()
dataset_collection = db["datasets"]

query_dict = {
    "selected": True,
    "annotation.tags": {
        "$all": ["approved"],
        "$nin": ["invalid", "ambiguous"]
    }
}

dataset_list = list(dataset_collection.find(query_dict))

nlp = spacy.load("en_core_web_sm")
file = open("redundant.csv", "w")
writer = csv.DictWriter(file, fieldnames=["cve_id", "sentence", "word"])
writer.writeheader()


class RedundantWordStripper:
    categories = ["cause", "location", "situation",
                  "version", "attacker", "operation", "consequence"]

    categories_dict = {categorie: {} for categorie in categories}

    def __init__(self, dataset) -> None:
        self.dataset = dataset
        self.id = dataset["_id"]
        self.annotations = dataset["annotation"]["annotations"]
        self.description = dataset["description"]

    def dup_annotations(self):
        part_list = []
        for annotation in self.annotations:
            part = annotation["part"]
            if part in part_list:
                return self.id
            else:
                part_list.append(part)
        return None


    def count_have(self):
        for annotation in self.annotations:
            start = annotation["start_offset"]
            end = annotation["end_offset"]
            part = annotation["part"]
            annotation_text_with_have: str = self.description[start-8:end]
            annotation_text_with_has: str = self.description[start-9:end]
            annotation_text_with_had: str = self.description[start-4:end]
            if part == "consequence":
                if annotation_text_with_had.startswith("had"):
                    return f"{self.id} {annotation_text_with_had}"
                # elif annotation_text_with_have.startswith("trigger"):
                #     return f"{self.id} {annotation_text_with_have}"
            # if part == "cause":
            #     if annotation_text_with_has.startswith("triggers"):
            #         return f"{self.id} {annotation_text_with_has}"
            #     elif annotation_text_with_has2.startswith("triggered"):
            #         return f"{self.id} {annotation_text_with_has2}"
            #     elif annotation_text_with_have.startswith("trigger"):
            #         return f"{self.id} {annotation_text_with_have}"

    def count_beginwords(self):
        for annotation in self.annotations:
            start = annotation["start_offset"]
            end = annotation["end_offset"]
            part = annotation["part"]
            annotation_text: str = self.description[start:end]
            doc = nlp(annotation_text.strip().split(" ")[0])
            token = doc[0]

            if not self.categories_dict[part].get(token.lemma_):
                self.categories_dict[part][token.lemma_] = []

            self.categories_dict[part][token.lemma_].append(self.id + ' ' +
                annotation_text[0:100]+"...")

    def strip_sentences(self):
        for annotation in self.annotations:
            start = annotation["start_offset"]
            end = annotation["end_offset"]
            annotation_text: str = self.description[start:end]
            doc = nlp(annotation_text)

            token = doc[0]

            if token.lemma_ in ["not"]:
                # if token.lemma_ in ["cause", "result", "lead", "in", "on"]:
                print(self.id + " ||| " + annotation_text + " ||| " + token.text)
                writer.writerow(
                    {"cve_id": self.id, "sentence": annotation_text, "word": token.text})

    def replace_annotation(self, old, new):
        update_dict = {
            "$pull": {"annotation.annotations": old},
            "$addToSet": {"annotation.annotations": new},
        }
        dataset_collection.find_one_and_update({"_id": self.id}, update_dict)


def detect_words(dataset):
    annotations = dataset["annotation"]["annotations"]
    description = dataset["description"]
    for annotation in annotations:
        start = annotation["start_offset"]
        end = annotation["end_offset"]

        annotation_text: str = description[start:end]

        doc = nlp(annotation_text)
        token = doc[0]
        if token.lemma_ in ["cause", "result", "lead"]:
            print(annotation_text)
            print(token)


with open("temp.txt", "w") as file:
    for dataset in dataset_list:
        ds = RedundantWordStripper(dataset)
        have = ds.count_have()
        if have:
            file.write(have+ "\n")

# for dataset in dataset_list:
#     ds = RedundantWordStripper(dataset)
#     ds.count_beginwords()

# for category_name, begin_words in RedundantWordStripper.categories_dict.items():
#     begin_words_tuple_list = [it for it in begin_words.items()]
#     sorted_list = sorted(begin_words_tuple_list, key=lambda key: len(key[1]),reverse=True)
#     with open("./stats/"+ category_name+".json", "w") as file:
#         jsonfy = json.dump(sorted_list, file, indent=4)

