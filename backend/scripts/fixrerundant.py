from base import *
import spacy
import csv

nlp = spacy.load("en_core_web_sm")
file = open("redundant.csv", "w")
writer = csv.DictWriter(file, fieldnames=["cve_id", "sentence", "word"])
writer.writeheader()


class RedundantWordStripper(Dataset):

    def count_beginwords(self):
        for annotation in self.annotations:
            start, end, part = annotation.start_offset, annotation.end_offset, annotation.part
            annotation_text: str = self.description[start:end]
            doc = nlp(annotation_text.strip().split(" ")[0])
            token = doc[0]

            if not self.categories_dict[part].get(token.lemma_):
                self.categories_dict[part][token.lemma_] = []

            self.categories_dict[part][token.lemma_].append(self.id + ' ' +
                                                            annotation_text[0:100]+"...")

    def strip_sentences(self):
        for annotation in self.annotations:
            start = annotation["start_offset"]
            end = annotation["end_offset"]
            annotation_text: str = self.description[start:end]
            doc = nlp(annotation_text)

            token = doc[0]

            if token.lemma_ in ["not"]:
                # if token.lemma_ in ["cause", "result", "lead", "in", "on"]:
                print(self.id + " ||| " + annotation_text + " ||| " + token.text)
                writer.writerow(
                    {"cve_id": self.id, "sentence": annotation_text, "word": token.text})

    def replace_annotation(self, old, new):
        update_dict = {
            "$pull": {"annotation.annotations": old},
            "$addToSet": {"annotation.annotations": new},
        }
        dataset_collection.find_one_and_update({"_id": self.id}, update_dict)


def detect_words(dataset):
    annotations = dataset["annotation"]["annotations"]
    description = dataset["description"]
    for annotation in annotations:
        start = annotation["start_offset"]
        end = annotation["end_offset"]

        annotation_text: str = description[start:end]

        doc = nlp(annotation_text)
        token = doc[0]
        if token.lemma_ in ["cause", "result", "lead"]:
            print(annotation_text)
            print(token)


# with open("temp.txt", "w") as file:
#     for dataset in dataset_list:
#         ds = RedundantWordStripper(dataset)
#         have = ds.count_have()
#         if have:
#             file.write(have+ "\n")

# for dataset in dataset_list:
#     ds = RedundantWordStripper(dataset)
#     ds.count_beginwords()

# for category_name, begin_words in RedundantWordStripper.categories_dict.items():
#     begin_words_tuple_list = [it for it in begin_words.items()]
#     sorted_list = sorted(begin_words_tuple_list, key=lambda key: len(key[1]),reverse=True)
#     with open("./stats/"+ category_name+".json", "w") as file:
#         jsonfy = json.dump(sorted_list, file, indent=4)
